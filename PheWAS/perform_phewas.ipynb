{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b145c8c3-f64f-463b-943d-b4c63dfbc306",
   "metadata": {},
   "source": [
    "# PheWAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd658773-3f21-4753-8fbf-e9110517bae8",
   "metadata": {},
   "source": [
    "==============================================================================\n",
    "<br>\n",
    "Author: Pierre-Raphael Schiratti <br>\n",
    "This code is adapted from Wenjia Bai's script on <br>\n",
    "Bai, W., Suzuki, H., Huang, J. et al. A population-based phenome-wide <br>\n",
    "association study of cardiac and aortic structure and function. Nat Med 26, <br>\n",
    "1654â€“1662 (2020). https://doi.org/10.1038/s41591-020-1009-y <br>\n",
    "=============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57d0696-e1c6-463e-9ac1-5753c46b18e4",
   "metadata": {},
   "source": [
    "<b> Perform a phenome-wide association study (PheWAS) on polygenic risk score (PRS) with non-imaging phenotypes selected from the UK Biobank dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1195b84",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import scipy.stats\n",
    "import math\n",
    "import re\n",
    "import rdata \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import warnings\n",
    "from my_fdr import fdr_threshold\n",
    "warnings.filterwarnings(action='ignore')\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e1b66d6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def normalise(x):\n",
    "    # Normalise variable\n",
    "    return (x - np.mean(x)) / np.std(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bba4bde",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def rank_normalise(x):\n",
    "    # Rank-based inverse normal transform\n",
    "    # Please refer to the function inormal() in the FSLNets package\n",
    "\n",
    "    # Get the rank of the values in x\n",
    "    ri = np.argsort(np.argsort(x))\n",
    "\n",
    "    # Correct for the ranks of repeated values\n",
    "    # argsort assign different ranks for these values\n",
    "    # We fill them with the same value\n",
    "    u, inv_idx = np.unique(x, return_inverse=True)\n",
    "    sii = np.sort(inv_idx)\n",
    "    repeated_idx = np.unique(sii[np.diff(np.append(sii, 1)) == 0])\n",
    "    for i in repeated_idx:\n",
    "        ri[inv_idx == i] = np.mean(ri[inv_idx == i])\n",
    "\n",
    "    # Perform inverse normal transform\n",
    "    # ri + 1 so that the rank starts from 1, to be consistent with Karla's Matlab code\n",
    "    # p squashes the rank into the range of [0, 1]\n",
    "    # erfinv can generate a distribution from 2 * p - 1 with 0 mean and 1 standard deviation\n",
    "    N = len(x)\n",
    "    ri = ri + 1\n",
    "    c = 3.0 / 8\n",
    "    p = (ri - c) / (N - 2 * c + 1)\n",
    "    y = math.sqrt(2) * scipy.special.erfinv(2 * p - 1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4ee7752",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def sizes_legend(desired_sizes, ax=None):\n",
    "    # Adjust size of plot legend\n",
    "    ax = ax or plt.gca()\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    labels = np.array([float(l) for l in labels])\n",
    "    desired_handles = [handles[np.argmin(np.abs(labels - d))] for d in desired_sizes]\n",
    "    ax.legend(handles=desired_handles, labels=desired_sizes, title=ax.legend_.get_title().get_text(),\n",
    "              loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a8a48bb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def roundup(x):\n",
    "    return int(math.ceil(x / 100.0)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9609e006-1364-45e5-a68a-b25ce550c428",
   "metadata": {},
   "source": [
    "### Parameters for script execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c1fef1b-0d6b-4bf0-9d41-bc5cc53ef177",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "write_csv = True # flag to save intermediary files\n",
    "plot = True # flag to plot data\n",
    "\n",
    "result_path = r\"./results\"\n",
    "data_path = r\"./data\"\n",
    "non_idp_path = r\"./data/my_ukbb_data_all_saved.rda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44487ea4-ba45-421c-a0d2-30090a0d37a0",
   "metadata": {},
   "source": [
    "### Read polygenic risk score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd9ec656-f679-455f-b2b4-9ee35a066f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prs = pd.read_csv(f'{data_path}/prs_pred.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9182b1e-a82d-4268-aec0-932fa2bb8a31",
   "metadata": {},
   "source": [
    "### Read non-imaging phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2e171ca-4146-474c-b80d-d3a72ebbbbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ukbb non-imaging phenotypes data file\n",
    "parsed = rdata.parser.parse_file(non_idp_path)\n",
    "converted = rdata.conversion.convert(parsed)\n",
    "df = converted['my_ukbb_data'].copy(deep=True)\n",
    "\n",
    "# Replace -2147483648 by None (importation error from RData)\n",
    "df = df.replace(-2147483648,np.nan)\n",
    "\n",
    "### Re-adapt ukbb data ###\n",
    "# A dataframe with fields and IDs from non-imaging phenotypes\n",
    "var = pd.DataFrame(data = df.columns, columns = ['name'])\n",
    "var['ukbb_id'] = var.name.str.extract('_f(\\d+)_\\d+')\n",
    "var = var.dropna(subset=['ukbb_id'])\n",
    "var.ukbb_id = var.ukbb_id.astype(int)\n",
    "var['rec'] = var.name.str.extract('_(\\d+_\\d+)$')\n",
    "\n",
    "# Catalogue of category + field from UKBB\n",
    "path_ukbb_field = rf\"{data_path}/ukbb_reco_cat_field.csv\"\n",
    "ukbb_field = pd.read_csv(path_ukbb_field, encoding='utf-8')\n",
    "ukbb_field.columns = ['cat_id', 'field_id', 'cat', 'field', 'cat_sup']\n",
    "ukbb_field = ukbb_field.drop_duplicates(subset='field_id',keep='first')\n",
    "\n",
    "# Match check between dataframe of variables and dataframe of definitions\n",
    "df_match = pd.merge(var, ukbb_field, how='inner', left_on='ukbb_id', right_on='field_id') \n",
    "# Create new column name having field, and UID (field_id + rec_id)\n",
    "df_match['new_id'] = df_match.ukbb_id.astype(str) + '-' + df_match.rec.str.replace('_','.')\n",
    "df_match['new_name'] = df_match.field + '_' + df_match.new_id\n",
    "# Order df_match by category name in ascending order, order of the plot\n",
    "df_match = df_match.sort_values(by=['cat','field_id'])\n",
    "if write_csv:\n",
    "    df_match.to_csv(f'{result_path}/ukbb_fields_description.csv')\n",
    "\n",
    "# Rename initial dataframe with new column names\n",
    "df = df[['eid'] + df_match.name.tolist()]\n",
    "df.columns = ['eid'] + df_match.new_name.values.tolist()\n",
    "# Redefine and rename index\n",
    "df.index = df.eid\n",
    "df = df.drop(columns='eid')\n",
    "df.index.names = ['eid_40616']\n",
    "# Only keep rows present in cardiac age dataframe and remove rows where sex is na\n",
    "df = df.loc[df.index.isin(df_prs.index)]\n",
    "df = df[df['Sex_31-0.0'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382fa316-66ca-49b5-a57a-9c8df0bb8eb3",
   "metadata": {},
   "source": [
    "### Confounding factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04736293-79a5-40f2-9149-2dcd9bcc99f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sex_31-0.0'] = df['Sex_31-0.0'].cat.codes.astype(int)\n",
    "sex = df['Sex_31-0.0']\n",
    "# Age provided by UK Biobank (21003-2.0) seems to be floored, i.e. with half a year error.\n",
    "# To get more accurate age values, we calculate age by date.\n",
    "month_dict = {\"January\": 1, \n",
    "              \"February\": 2, \n",
    "              \"March\": 3, \n",
    "              \"April\": 4, \n",
    "              \"May\": 5, \n",
    "              \"June\": 6, \n",
    "              \"July\": 7, \n",
    "              \"August\": 8, \n",
    "              \"September\": 9, \n",
    "              \"October\": 10, \n",
    "              \"November\": 11, \n",
    "              \"December\": 12}\n",
    "df['Month of birth_52-0.0'] = df['Month of birth_52-0.0'].replace(month_dict)\n",
    "df['Date of attending assessment centre_53-2.0'] = pd.to_datetime(df['Date of attending assessment centre_53-2.0'],unit='D', origin='1970-1-1').astype(str)\n",
    "age = np.zeros(len(df))\n",
    "for i in range(len(df)):\n",
    "    # Calculate age\n",
    "    try:\n",
    "        d1 = datetime.date(df.iloc[i]['Year of birth_34-0.0'], df.iloc[i]['Month of birth_52-0.0'], 15)\n",
    "        s = df.iloc[i]['Date of attending assessment centre_53-2.0']\n",
    "        d2 = datetime.date(int(s[:4]), int(s[5:7]), int(s[8:]))\n",
    "        age[i] = np.round((d2 - d1).days / 365.25, 1)\n",
    "    except:\n",
    "        age[i] = df.iloc[i]['Age when attended assessment centre_21003-2.0']\n",
    "\n",
    "# Keep the rows with age, sex information\n",
    "valid_idx = ~np.isnan(age) & ~np.isnan(sex)\n",
    "df = df[valid_idx]\n",
    "df_prs = df_prs[df_prs.index.isin(valid_idx.index)]\n",
    "df_prs = df_prs[~df_prs.index.duplicated()]\n",
    "sex = sex[valid_idx]\n",
    "age = age[valid_idx]\n",
    "age_age = age * age  # Age squared interaction\n",
    "\n",
    "# Confounding factors: sex, age, age_age\n",
    "conf = np.stack((sex, age, age_age), axis=1)\n",
    "df_conf = pd.DataFrame(conf, index=df.index, columns=['Sex', 'Age', 'Age * Age'])\n",
    "if write_csv:\n",
    "    df_conf.to_csv(f'{result_path}/confounders_26k.csv')\n",
    "\n",
    "# Remove confounding factors from df\n",
    "df = df.drop(columns=[('Sex_31-0.0'),\n",
    "                      ('Year of birth_34-0.0'),\n",
    "                      ('Month of birth_52-0.0'),\n",
    "                      ('Date of attending assessment centre_53-0.0'),\n",
    "                      ('Date of attending assessment centre_53-1.0'),\n",
    "                      ('Date of attending assessment centre_53-2.0'),\n",
    "                      ('Age when attended assessment centre_21003-0.0'),\n",
    "                      ('Age when attended assessment centre_21003-1.0'),\n",
    "                      ('Age when attended assessment centre_21003-2.0'),\n",
    "                      ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4f834c-c762-49fc-a4e0-810dfde917b4",
   "metadata": {},
   "source": [
    "### Clean, de-confound and normalise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51d0ba95-813a-4942-9b7b-d1ccaeb6fc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380 columns kept after data cleaning.\n",
      "Were not in continuous file: []\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 37 but corresponding boolean dimension is 486",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 96>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m valid_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mnp\u001b[38;5;241m.\u001b[39misnan(val)\n\u001b[1;32m     99\u001b[0m x \u001b[38;5;241m=\u001b[39m val[valid_idx]\n\u001b[0;32m--> 100\u001b[0m beta[:, i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mpinv(\u001b[43mconf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalid_idx\u001b[49m\u001b[43m]\u001b[49m), x)\n\u001b[1;32m    101\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(conf[valid_idx], beta[:, i])\n\u001b[1;32m    102\u001b[0m x \u001b[38;5;241m=\u001b[39m normalise(x)\n",
      "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 37 but corresponding boolean dimension is 486"
     ]
    }
   ],
   "source": [
    "# This part of the code was adapted from Karla Miller's Matlab code at\n",
    "# https://www.fmrib.ox.ac.uk/ukbiobank/gwaspaper/index.html\n",
    "# Step 4.1: cleaning\n",
    "n_subj, n_col = df.shape\n",
    "bad_vars = []\n",
    "for i in range(n_col):\n",
    "    val = df.iloc[:, i]\n",
    "\n",
    "    # Discard columns which are not numbers\n",
    "    if df.iloc[:, i]._get_numeric_data().empty:\n",
    "        bad_vars += [i]\n",
    "        continue\n",
    "\n",
    "    # Assume negative values are invalid, set them to NaN\n",
    "    # There are also a lot of empty values, which are already NaN\n",
    "    val[val < 0] = np.nan\n",
    "    df.iloc[:, i] = val\n",
    "\n",
    "    # Valid indices\n",
    "    valid_idx = ~np.isnan(val)\n",
    "\n",
    "    # Discard columns with more than 90% missing data\n",
    "    if np.sum(valid_idx) < (0.1 * n_subj):\n",
    "        bad_vars += [i]\n",
    "        continue\n",
    "\n",
    "    # Discard columns with over 95% elements with the exactly same value\n",
    "    val_unique, counts = np.unique(val[valid_idx], return_counts=True)\n",
    "    if np.max(counts) >= (0.95 * np.sum(valid_idx)):\n",
    "        bad_vars += [i]\n",
    "        continue\n",
    "\n",
    "for i in range(n_col):\n",
    "    for j in range(i + 1, n_col):\n",
    "        if i in bad_vars or j in bad_vars:\n",
    "            continue\n",
    "        # Discard columns with very high correlation\n",
    "        val_i = df.iloc[:, i]\n",
    "        val_j = df.iloc[:, j]\n",
    "        valid_idx = ~np.isnan(val_i) & ~np.isnan(val_j)\n",
    "        if np.sum(valid_idx) < 2: # was ==0 before\n",
    "            continue\n",
    "        cc, _ = scipy.stats.pearsonr(val_i[valid_idx], val_j[valid_idx])\n",
    "        if cc > 0.9999:\n",
    "            # Keep the column with more valid elements\n",
    "            if np.sum(~np.isnan(val_i)) > np.sum(~np.isnan(val_j)):\n",
    "                bad_vars += [j]\n",
    "            else:\n",
    "                bad_vars += [i]\n",
    "\n",
    "# The cleaned data\n",
    "bad_vars = np.unique(bad_vars)\n",
    "keep_vars = sorted(list(set(np.arange(n_col)) - set(bad_vars)))\n",
    "df = df.iloc[:, keep_vars]\n",
    "print('{0} columns kept after data cleaning.'.format(df.shape[1]))\n",
    "\n",
    "# Step 4.2: normalise confounding factors\n",
    "conf = (conf - np.mean(conf, axis=0)) / np.std(conf, axis=0)\n",
    "\n",
    "# Step 4.3: normalise non imaging phenotypes\n",
    "df_cont = pd.read_csv(f\"{data_path}/continuous.csv\", index_col=0)\n",
    "\n",
    "n_col = df.shape[1]\n",
    "not_in = []\n",
    "for i in range(n_col):\n",
    "    val = df.iloc[:, i]\n",
    "    valid_idx = ~np.isnan(val)\n",
    "    x = val[valid_idx]\n",
    "\n",
    "    # Field ID\n",
    "    field_id = int(re.search(r'_([0-9]*)-',df.columns[i]).group(1))\n",
    "\n",
    "    # some are not in the continuous file?\n",
    "    if field_id not in df_cont.index:\n",
    "        is_continuous=False\n",
    "        not_in.append(field_id)\n",
    "        continue\n",
    "    is_continuous = df_cont.loc[field_id]['continuous']\n",
    "\n",
    "    if is_continuous:\n",
    "        # If it is a continuous variable, perform standard normalisation.\n",
    "        x = normalise(x)\n",
    "    else:\n",
    "        # If we are not sure whether it is a continuous or categorical variable, convert it\n",
    "        # into a continuous variable using rank-based inverse normal transform.\n",
    "        x = rank_normalise(x)\n",
    "    df.iloc[:, i][valid_idx] = x\n",
    "print(f'Were not in continuous file: {not_in}')\n",
    "if write_csv:\n",
    "    df.to_csv(f'{result_path}/normalised_non_IDPs_26k.csv')\n",
    "\n",
    "# Step 4.4: de-confound and normalise IDPs+\n",
    "n_row = conf.shape[1]\n",
    "n_col = df_prs.shape[1]\n",
    "beta = np.zeros((n_row, n_col))\n",
    "for i in range(n_col):\n",
    "    val = df_prs.iloc[:, i]\n",
    "    valid_idx = ~np.isnan(val)\n",
    "    x = val[valid_idx]\n",
    "    beta[:, i] = np.dot(np.linalg.pinv(conf[valid_idx]), x)\n",
    "    x = x - np.dot(conf[valid_idx], beta[:, i])\n",
    "    x = normalise(x)\n",
    "    df_prs.iloc[:, i][valid_idx] = x\n",
    "df_beta = pd.DataFrame(beta,\n",
    "                        index=['sex', 'age', 'sex * age'],\n",
    "                        columns=df_prs.columns)\n",
    "if write_csv:\n",
    "    df_prs.to_csv(f'{result_path}/normalised_delta_26k.csv')\n",
    "    df_beta.to_csv(f'{result_path}/beta_delta_26k.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab42902-a050-4f1d-bf24-70fd1cd8e169",
   "metadata": {},
   "source": [
    "### Uni-variate correlation studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fa85c9-c765-461f-96b6-458baf53c8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = df_prs.shape[1]\n",
    "N = df.shape[1]\n",
    "corr = np.zeros((M, N))\n",
    "corr_p = np.zeros((M, N))\n",
    "for i in range(M):\n",
    "    for j in range(N):\n",
    "        # Remove NaNs\n",
    "        x = df_prs.iloc[:, i]\n",
    "        y = df.iloc[:, j]\n",
    "        valid_idx = ~np.isnan(x) & ~np.isnan(y)\n",
    "        x = x[valid_idx]\n",
    "        y = y[valid_idx]\n",
    "\n",
    "        # Pearson correlation\n",
    "        cc, p_val = scipy.stats.pearsonr(x, y)\n",
    "        corr[i, j] = cc\n",
    "        corr_p[i, j] = p_val\n",
    "\n",
    "# For p-value of 0, assign it with the mininal positive floating value\n",
    "# so that we can calculate the logarithm for the Manhattan plot\n",
    "corr_p[corr_p == 0] = np.finfo(np.float64).tiny\n",
    "\n",
    "# Logarithm\n",
    "log_corr_p = - np.log10(corr_p)\n",
    "\n",
    "# Save the table\n",
    "df_corr = pd.DataFrame(np.transpose(np.concatenate([corr, corr_p, log_corr_p])), index=df.columns, columns=['corr','corr_p','log_corr_p'])\n",
    "if write_csv:\n",
    "    df_corr.to_csv(f'{result_path}/table_corr_26k.csv')\n",
    "\n",
    "# Bonferroni correction\n",
    "M, N = corr.shape\n",
    "p_bonf = 0.05 / (M * N)\n",
    "\n",
    "# FDR correction\n",
    "p_fdr, _ = fdr_threshold(corr_p.flatten(), 0.05)\n",
    "\n",
    "# Number of phenotypes that is significantly associated with at least one of the IDPs\n",
    "print('p_bonf = {0}'.format(p_bonf))\n",
    "print('p_fdr = {0}'.format(p_fdr))\n",
    "print('Number of correlations reaching Bonferroni threshold = {0}'.format(np.sum(corr_p < p_bonf)))\n",
    "print('Number of correlations reaching FDR threshold = {0}'.format(np.sum(corr_p < p_fdr)))\n",
    "print('Number of phenotypes reaching Bonferroni threshold = {0}'.format(np.sum(np.sum(corr_p < p_bonf, axis=0) > 0)))\n",
    "print('Number of phenotypes reaching FDR threshold = {0}'.format(np.sum(np.sum(corr_p < p_fdr, axis=0) > 0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b395e5c7-719c-4365-90c2-fffdce0ef408",
   "metadata": {},
   "source": [
    "### Manhattan plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148bf018",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot:\n",
    "    # Category for each field, ordered for the plot\n",
    "    category = []\n",
    "    for field_id in df.columns.str.extract(r'_([0-9]*)-').values:\n",
    "        category += [ukbb_field.loc[ukbb_field.field_id==int(field_id),'cat']]\n",
    "    category = np.array(category)\n",
    "\n",
    "    # Dataframe with fields and associated correlations\n",
    "    table = []\n",
    "    s = 'Cardiac age delta'\n",
    "    for j in range(N):\n",
    "        line = [j, log_corr_p[0, j], corr[0, j], abs(corr[0, j]), s]\n",
    "        table += [line]\n",
    "    df_table = pd.DataFrame(table, columns=['x', 'log_p', 'corr', 'Correlation', 'Feature'])\n",
    "\n",
    "    # Run for correlation plot and log plot\n",
    "    var_d = {'log_p':{'y':'log_p',\n",
    "                      'size':'Correlation',\n",
    "                      'ylabel':r'$-\\log_{10}(p)$',\n",
    "                      'ylim_inf':0,\n",
    "                      'ylim_sup':roundup(df_table.log_p.max()),\n",
    "                      'size_min':-0.01,\n",
    "                      'size_max':0.3,\n",
    "                      'last_label':'0.3',\n",
    "                      'desired_sizes':[0,0.1,0.2,0.3]\n",
    "                      },\n",
    "             'corr':{'y':'corr',\n",
    "                     'size':'log_p',\n",
    "                     'ylabel':r'Correlation coefficient',\n",
    "                     'ylim_inf':-0.4,\n",
    "                     'ylim_sup':0.4,\n",
    "                     'size_min':0,\n",
    "                     'size_max':300,\n",
    "                     'last_label':'300',\n",
    "                     'desired_sizes':[0,100,200,300]\n",
    "                     }\n",
    "             }\n",
    "\n",
    "    for k,v in var_d.items():\n",
    "        plt.figure()\n",
    "        ax = sns.scatterplot(x='x', y=v['y'], size=v['size'],\n",
    "                             sizes=(20,200), \n",
    "                             size_norm=mpl.colors.Normalize(vmin=v['size_min'], vmax=v['size_max']),\n",
    "                             data=df_table, alpha=0.8)\n",
    "        if k=='log_p':\n",
    "            plt.plot([0, N], [-math.log10(p_bonf), -math.log10(p_bonf)], 'k--', linewidth=1, alpha=0.8)\n",
    "            plt.text(-1, -math.log10(p_bonf), 'Bonf', horizontalalignment='right', fontsize=11)\n",
    "        else:\n",
    "            plt.plot([0, N], [0, 0], 'k-', linewidth=1, alpha=0.8)\n",
    "        sizes_legend(v['desired_sizes'], ax)\n",
    "\n",
    "        xticks = []\n",
    "        xticklabels = []\n",
    "        unique_category = ukbb_field.cat.unique()\n",
    "        unique_category.sort()\n",
    "        c_last = unique_category[-1]\n",
    "        for c in unique_category:\n",
    "            cid = np.nonzero(category == c)[0]\n",
    "            if len(cid)==0:\n",
    "                continue\n",
    "            x = np.max(cid) + 0.5\n",
    "            if c != c_last:\n",
    "                plt.plot([x, x], [v['ylim_inf'], v['ylim_sup']], 'k--', linewidth=0.5)\n",
    "            xticks += [np.mean(cid)]\n",
    "            xticklabels += [c]\n",
    "\n",
    "        plt.xlim(0, N)\n",
    "        plt.ylim(v['ylim_inf'], v['ylim_sup'])\n",
    "        plt.xticks(xticks, xticklabels, fontsize=10, rotation=45)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.xlabel('Non-imaging phenotypes', fontsize=12)\n",
    "        plt.ylabel(v['ylabel'], fontsize=12)\n",
    "        plt.title('Phenome-wide association study on polygenic risk score',fontsize=20)\n",
    "        fig = plt.gcf()\n",
    "        fig.set_size_inches(16, 8)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{result_path}/manhattan_plot_{k}.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
